#### Distilling the Knowledge in a Neural Network
##### Abstract
&emsp;&emsp; 使用不同模型集成可以提高性能, 但是这样做的成本太高. 使用压缩技术将集成模型的知识压缩为单一模型. 本文也提出了不同的模型集成方法.
##### 1. Introduction
&emsp;&emsp; 当一个庞大的模型被训练完后,我们可以使用一个较小的模型, 通过蒸馏技术将大模型的知识迁移到小模型中.
&emsp;&emsp; 一大障碍在于, 我们通常通过一个模型的学习到的参数来定义这个模型学到的知识, 这让我们很难在保留模型的知识的前提下改变模型的形式.**对于模型的参数, 我们难以找到直观的方式进行迁移学习处理, 但是可以将模型抽象为为f(输入)=输出, 因此我们可以在模型的输出上进行迁移学习** 以分类任务为例, 模型通常最大化正确答案的对数概率, 对然在优化时只对正样本进行优化, 但模型对所有的结果都会给出一个概率,  并且该概率可以体现模型的泛化性, 例如错误越明显的例子得到的概率越低. 因此一种**直观**的, 从大模型迁移学习到小模型的方法, 是**将大模型的输出概率作为小模型训练时的soft target软标签**.
&emsp;&emsp; 对于某些数据集例如MNIST, 大模型近乎于能够完全正确预测, 他给出的概率分布具有压倒性的分布,例如 正确类别概率接近于1, 错误类别概率接近于0. 但是不同错误类别之间的概率存在着差距, 这些差距因为数量级太小显得不明显, 因此需要使用某些手段来利用这些信息. 有人使用logits而不是softmax来避免这个问题, 本文使用蒸馏技术(scaled-softmax).
&emsp;&emsp; 迁移过程中可以使用大模型的训练集进行训练, 也可以使用额外的数据集进行训练, 值得注意的是, **训练数据集可以是无标签的**, **仅通过大模型的概率分布来对小模型进行优化**. 通常使用大模型的数据集进行训练效果会好. 如果在目标函数中加入额外的一项, 让小模型的预测和大模型的概率软标签相似, 会进一步提高结果.

##### 2. Distillation
&emsp;&emsp; 机器学习通常使用softmax来得到每个类别的概率, 但可能会导致概率之间相差悬殊, 因此本文在softmax上进行了缩放, 添加了温度超参数.(SimCSE的损失函数也是这么做的)
$$
q_i = \frac
	{exp(z_i/T)}
	{\sum_jexp(z_j/T)}
$$
&emsp;&emsp;这就是最简单的蒸馏形式, 使用大模型经过缩放后得到的软概率标签分布来训练小模型, 在训练过程中使用大的温度超参数, 使得软标签分布变得"不相差悬殊", 在迁移结束后恢复原始的softmax.
&emsp;&emsp; 如果在无监督迁移的基础上知道某些数据样例的正确标签, 可以显著提高迁移训练的表现. 一种方法是, 使用正确的标签来修改软标签, 但是我们发现一种更好的方法是将两个目标函数进行加权平均计算损失. <u>第一个</u>目标函数是和软标签之间计算交叉熵(pytorch中使用kl_div), **要注意的是, 这里交叉熵的softmax概率要使用和大模型生成软标签时的softmax同样的较高的温度参数.** <u>第二个</u>目标函数时和正确标签之间的交叉熵损失, 这里使用标准的softmax来计算概率. 作者发现, 使用这两个目标函数的加权, 并且给与第二个目标较小的权重可以取得较好的结果. 由于使用温度系数对软标签进行缩放,  软标签算出来的梯度也会被缩放为$1/T^2$, 为了保证软标签和硬标签的贡献一致,  要将软标签的梯度乘以$T^2$倍.

##### 5. Training ensembles  on very big datasets
&emsp;&emsp; 大的集成模型在推理过程中需要耗费大量时间可以通过迁移学习一个小的模型来改善， 尽管如此，在训练过程中的大量算力需求也是难以达到的。为了解决训练过程困难的问题，本文提出了一种方法， 让每个专家模型都关注于所有类别的不同的可以混淆的子集类别，可以减少学习集成模型的耗时。但是，过于细粒度的区分专家模型要聚焦的类别容易造成过拟合，作者使用soft target来减轻过拟合。
###### 5.3 Assigning classes to specialists
&emsp;&emsp;如何给每个专家模型分配类别呢？作者希望把容易混淆（相近）的类分配个同一个专家模型，于是作者使用聚类方法。在通用模型的预测协方差矩阵上使用聚类方法，将经常被预测到一起的类别作为一个子类分配给一个专家模型。
###### 5.4 Performing inference with ensembles of specilists
&emsp;&emsp; 在对集成模型使用只是蒸馏之前，首先来看一下直接用集成模型进行推理的表现如何。除了专家模型外，还有一个通用模型，该模型能够对所有的类别进行分类。Step1:对每一个测试样例, 根据通用模型找到n个最可能的类别, 记这组类别为k. Step2: