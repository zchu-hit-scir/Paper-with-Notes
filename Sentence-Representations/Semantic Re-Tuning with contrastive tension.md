#### Semantic Re-Tuning with Contrastive Tension - RISE NLU
##### Abstract
&emsp;&emsp; 本文首次通过对Transformer预训练模型在不同层上STS相关性的调查发现, 训练目标**对模型的最后几层施加了显著的任务偏置**.  本文提出了Contrastive Tension来对抗这个偏差. CT将训练目标设定为**两个独立模型最后一层表征的噪声对比任务**.

##### 3. Layer-wise Study of Transformer Models
